[hytrel]
hidden_size=1024
intermediate_size=4096
num_attention_heads=16
num_hidden_layers=12

[gformer]
strategy=v3

num_query_tokens=32
model_name_or_path="FacebookAI/roberta-base"

freeze_encoder=False
model_finetuning_type="full"

# ckpt_path=

[llm]
model_name_or_path="meta-llama/Llama-2-7b-hf"

attn_implementation="flash_attention_2"

finetuning_type="freeze"
