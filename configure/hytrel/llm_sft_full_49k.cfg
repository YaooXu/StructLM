[gformer]
ckpt_path = new-outputs/data/pretraining_25M_tables/hytrel/gformer_pretraining_768_12cross_wd.cfg/checkpoint-49456/gformer.bin
freeze_encoder = False
model_finetuning_type = full
model_name_or_path = FacebookAI/roberta-base
num_query_tokens = 32
strategy = v3

[hytrel]
hidden_size = 768
intermediate_size = 3072
num_attention_heads = 12
num_hidden_layers = 12
return_all_layer = True

[llm]
attn_implementation = flash_attention_2
finetuning_type = "full"
model_name_or_path = mistralai/Mistral-7B-Instruct-v0.2

