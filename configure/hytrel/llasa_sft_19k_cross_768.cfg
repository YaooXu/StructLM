[hytrel]
hidden_size=768
intermediate_size=3072
num_attention_heads=12
num_hidden_layers=12

return_all_layer=True

[gformer]
strategy=v3

num_query_tokens=32
model_name_or_path="FacebookAI/roberta-base"

freeze_encoder=False
model_finetuning_type="full"

ckpt_path=new-outputs/data/pretraining_10M_tables/hytrel/gformer_pretraining_768_12cross.cfg/checkpoint-19530/gformer.bin

[llm]
model_name_or_path="mistralai/Mistral-7B-Instruct-v0.2"

attn_implementation="flash_attention_2"

finetuning_type="lora"
target_modules=q_proj,k_proj,v_proj,o_proj
r=32
lora_alpha=64
lora_dropout=0

# ckpt_path=
